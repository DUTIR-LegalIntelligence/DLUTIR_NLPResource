{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data.dataset import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.py', 'data_load.py', 'model.py', 'train.py', 'data_handle.ipynb', 'checkpoints', 'temp.txt', '__pycache__', '.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/train.json','rt',encoding='utf-8') as fin1,open('../data/test.json','rt',encoding='utf-8') as fin2,\\\n",
    "open('../data/train.txt','w',encoding='utf-8') as fout1,open('../data/valid.txt','w',encoding='utf-8') as fout2,open('../data/test.txt','w',encoding='utf-8') as fout3:\n",
    "    labels=set()\n",
    "    train_=[]\n",
    "    for line in fin1:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        \n",
    "        train_.append(sentence)\n",
    "    train_size=len(train_)\n",
    "    val_size = int(0.25 * len(train_))\n",
    "    train_,val_= random_split(train_, [train_size-val_size,val_size])\n",
    "    train_=list(train_)\n",
    "    val_=list(val_)\n",
    "\n",
    "    for tr in train_:\n",
    "        sentence_text=list(tr[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        label=['O' for i in range(len(sentence_text))]\n",
    "#         print(sentence_text[112])\n",
    "        for e in tr['entityMentions']:\n",
    "            labels.add(\"B-\"+e['label'])\n",
    "            labels.add(\"I-\"+e['label'])\n",
    "            label[e['start']]='B-'+e['label']\n",
    "            for i in range(e['start']+1,e['end']):\n",
    "                label[i]='I-'+e['label']\n",
    "        for j in range(len(label)):\n",
    "            fout1.write(sentence_text[j]+'\\t'+label[j]+'\\n')\n",
    "        fout1.write('\\n')\n",
    "#             print(sentence_text[j],label[j])\n",
    "        #break\n",
    "    \n",
    "    for vl in val_:\n",
    "        sentence_text=list(vl[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        label=['O' for i in range(len(sentence_text))]\n",
    "#         print(sentence_text[112])\n",
    "        for e in vl['entityMentions']:\n",
    "            labels.add(\"B-\"+e['label'])\n",
    "            labels.add(\"I-\"+e['label'])\n",
    "            label[e['start']]='B-'+e['label']\n",
    "            for i in range(e['start']+1,e['end']):\n",
    "                label[i]='I-'+e['label']\n",
    "        for j in range(len(label)):\n",
    "            fout2.write(sentence_text[j]+'\\t'+label[j]+'\\n')\n",
    "        fout2.write('\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    test_=[]\n",
    "    for line in fin2:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        test_.append(sentence)\n",
    "    \n",
    "    for te in test_:\n",
    "        sentence_text=list(te[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        label=['O' for i in range(len(sentence_text))]\n",
    "#         print(sentence_text[112])\n",
    "        for e in te['entityMentions']:\n",
    "            labels.add(\"B-\"+e['label'])\n",
    "            labels.add(\"I-\"+e['label'])\n",
    "            label[e['start']]='B-'+e['label']\n",
    "            for i in range(e['start']+1,e['end']):\n",
    "                label[i]='I-'+e['label']\n",
    "        for j in range(len(label)):\n",
    "            fout3.write(sentence_text[j]+'\\t'+label[j]+'\\n')\n",
    "        fout3.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('O',\n",
       " 'I-Ns',\n",
       " 'I-NDR',\n",
       " 'I-NW',\n",
       " 'B-Nh',\n",
       " '<pad>',\n",
       " 'B-NT',\n",
       " 'B-NW',\n",
       " 'B-Ns',\n",
       " 'B-NDR',\n",
       " 'I-NT',\n",
       " 'I-Nh')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.add('O')\n",
    "labels.add('<PAD>')\n",
    "tuple(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries = open('conll2003/test.txt', 'r').read().strip().split(\"\\n\\n\")\n",
    "sents, tags_li = [], [] # list of lists\n",
    "for entry in entries:\n",
    "    words = [line.split()[0] for line in entry.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from my_data_load import pad,NerDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "batch = NerDataset(\"data2/valid.txt\")\n",
    "maxl=0\n",
    "\n",
    "f = lambda x: [sample[x] for sample in batch]\n",
    "for l in f(-1):\n",
    "    l=max(l,maxl)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=[1] + [0]*(20 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data2/test_inf.txt','rt',encoding='utf-8') as fin1,open('data2/my_test_inf.txt','w',encoding='utf-8') as fout1:\n",
    "    \n",
    "    train_=[]\n",
    "    for line in fin1:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环    \n",
    "        train_.append(line)\n",
    "    \n",
    "    for tr in train_:\n",
    "        sentence_text=list(tr.strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        label=['O' for i in range(len(sentence_text))]\n",
    "#         print(sentence_text[112])\n",
    "        \n",
    "        for j in range(len(label)):\n",
    "            fout1.write(sentence_text[j]+'\\t'+label[j]+'\\n')\n",
    "        fout1.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lmax=0\n",
    "entries = open('data2/train.txt', 'r',encoding='utf-8').read().strip().split(\"\\n\\n\")\n",
    "for entry in entries:\n",
    "    lmax=max(lmax,len(entry.splitlines()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "486"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "###创建pipeline中关系抽取测试集\n",
    "path='../data/ner_result.tsv'\n",
    "with open(path, 'w') as fout:\n",
    "    result = open(\"temp.txt\", \"r\").read().strip().split(\"\\n\\n\")\n",
    "    \n",
    "    wtest=csv.writer(fout,delimiter='\\t')\n",
    "    for ID,entry in enumerate(result):\n",
    "        entities=[]\n",
    "        entity={}\n",
    "        words = [line.split()[0] for line in entry.splitlines()]\n",
    "        tags = ([line.split()[-1] for line in entry.splitlines()])\n",
    "#         print(words,tags)\n",
    "        for idx,t in enumerate(tags):\n",
    "#             print(t)\n",
    "            if t=='O':\n",
    "                if entity!={}:\n",
    "                    entity['end']=idx\n",
    "                    entities.append(entity)\n",
    "                    entity={}   #type start end text\n",
    "                continue\n",
    "            elif t.startswith('B'): \n",
    "                entity['start']=idx\n",
    "                entity['text']=words[idx]\n",
    "                entity['type']=t.split('-')[1]\n",
    "            elif t.startswith('I'):\n",
    "                if'text' in entity:\n",
    "                    entity['text']+=words[idx]\n",
    "            \n",
    "        entitypair_list=[]\n",
    "        for e1 in entities:\n",
    "            if e1['type']!='Nh' and e1['type']!='NDR':\n",
    "                continue\n",
    "            for e2 in entities:\n",
    "                if e2['type']!='Nh' and e2['type']!='NDR':\n",
    "                    continue\n",
    "                if e1!=e2 and e1['text']!=e2['text']:\n",
    "                    entitypair=[]\n",
    "                    entitypair.append(e1)\n",
    "                    entitypair.append(e2)\n",
    "                    if entitypair not in entitypair_list:\n",
    "                        entitypair_list.append(entitypair)\n",
    "        \n",
    "        for sid,ep in enumerate(entitypair_list):\n",
    "            sent=words.copy()\n",
    "            e1start=ep[0]['start']\n",
    "            e1end=ep[0]['end']\n",
    "            e2start=ep[1]['start']\n",
    "            e2end=ep[1]['end']\n",
    "            \n",
    "            if e1end>400 or e2end>400:\n",
    "                continue\n",
    "                \n",
    "            if e1start<e2start:\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "            else:\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                \n",
    "            sent=''.join(sent)\n",
    "#             sent_sub=re.sub(u'((?:经审理查明|[\\u4e00-\\u9fa5]*指控)(?:，|：))','',sent)    \n",
    "            wtest.writerow((sid,sent,0,ID))\n",
    "#         print(entitypair_list,len(entitypair_list))\n",
    "#         print(words)\n",
    "#         print(tags)\n",
    "#         print(entities)\n",
    "      \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jointextraction]",
   "language": "python",
   "name": "conda-env-jointextraction-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
