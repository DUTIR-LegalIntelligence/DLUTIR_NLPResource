{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from torch.utils.data.dataset import random_split\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##将反向关系加入负例中  在1:2基础上\n",
    "with open('../data/train_data.json','rt',encoding='utf-8') as fin1,open('../data/test_data.json','rt',encoding='utf-8') as fin2,\\\n",
    "open('../data/train_new.json','w',newline='',encoding='utf-8') as fout1,open('../data/test_new.json','w',newline='',encoding='utf-8') as fout2:\n",
    "    train_=[]\n",
    "    for line in fin1:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        train_.append(sentence)\n",
    "\n",
    "    for tr in train_:\n",
    "        sentence_text=list(tr[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        entity_list=[]\n",
    "        entitypair_list=[]\n",
    "        entity_list_other=[]\n",
    "        entitypair_list_other=[]\n",
    "        for entity in tr[\"entityMentions\"]:\n",
    "            entity_list_other.append((entity['text'],entity['start']))\n",
    "            if entity['label']=='Nh' or entity['label']=='NDR':\n",
    "                \n",
    "                entity_list.append((entity['text'],entity['start']))\n",
    "            \n",
    "            \n",
    "                \n",
    "#             entity_list.append((entity['text'],entity['start']))\n",
    "        for e1 in entity_list_other:\n",
    "            for e2 in entity_list_other:\n",
    "                if e1!=e2:\n",
    "                    entitypair=set()\n",
    "                    entitypair.add(e1)\n",
    "                    entitypair.add(e2)\n",
    "                    if entitypair not in entitypair_list_other:\n",
    "                        entitypair_list_other.append(entitypair)\n",
    "#         print(len(entitypair_list))\n",
    "        for e1 in entity_list:\n",
    "            for e2 in entity_list:\n",
    "                if e1!=e2:\n",
    "                    entitypair=set()\n",
    "                    entitypair.add(e1)\n",
    "                    entitypair.add(e2)\n",
    "                    if entitypair not in entitypair_list:\n",
    "                        entitypair_list.append(entitypair)\n",
    "                        \n",
    "        for relation in tr['relationMentions']:\n",
    "            if relation['label']!='NA':\n",
    "                sent=sentence_text.copy()\n",
    "\n",
    "                e1=relation['em1Text']\n",
    "                e2=relation['em2Text']\n",
    "                re=relation['label']\n",
    "\n",
    "                re_pair=set()\n",
    "                re_pair.add(e1)\n",
    "                re_pair.add(e2)\n",
    "                \n",
    "                if re_pair in entitypair_list:\n",
    "                    entitypair_list.remove(re_pair)\n",
    "                \n",
    "                tr['relationMentions'].append({\"e1start\":relation['e21start'],\\\n",
    "                                               \"em1Text\":relation['em2Text'],\\\n",
    "                                               \"e21start\":relation['e1start'],\\\n",
    "                                               \"em2Text\":relation['em1Text'],\\\n",
    "                                               \"label\":'NA'})\n",
    "#             wtrain.writerow((e1,e2,re,''.join(sent)))\n",
    "            #print(sid,''.join(sent),re_type,sentID)\n",
    "    \n",
    "#             print(sentence_text[j],label[j])\n",
    "#             if re not in relation_list:\n",
    "#                 relation_list.append(re)\n",
    "#         print(len(entitypair_list))\n",
    "#         if len(entitypair_list_other)>2:\n",
    "        \n",
    "#             ranlist=random.sample(entitypair_list_other,2)\n",
    "#             for ran in ranlist:\n",
    "#                 tr['relationMentions'].append({\"e1start\":list(ran)[0][1],\\\n",
    "#                                                \"em1Text\":list(ran)[0][0],\\\n",
    "#                                                \"e21start\":list(ran)[1][1],\\\n",
    "#                                                \"em2Text\":list(ran)[1][0],\\\n",
    "#                                                \"label\":'NA'})\n",
    "        if len(entitypair_list)>5:\n",
    "            ranlist=entitypair_list\n",
    "            ranlist=random.sample(entitypair_list,5)\n",
    "            for ran in ranlist:\n",
    "                tr['relationMentions'].append({\"e1start\":list(ran)[0][1],\\\n",
    "                                               \"em1Text\":list(ran)[0][0],\\\n",
    "                                               \"e21start\":list(ran)[1][1],\\\n",
    "                                               \"em2Text\":list(ran)[1][0],\\\n",
    "                                               \"label\":'NA'})\n",
    "#         else:\n",
    "#             for ran in entitypair_list:\n",
    "#                 tr['relationMentions'].append({\"e1start\":list(ran)[0][1],\\\n",
    "#                                                \"em1Text\":list(ran)[0][0],\\\n",
    "#                                                \"e21start\":list(ran)[1][1],\\\n",
    "#                                                \"em2Text\":list(ran)[1][0],\\\n",
    "#                                                \"label\":'NA'})\n",
    "\n",
    "        fout1.write(json.dumps(tr,ensure_ascii=False)+'\\n')\n",
    "      \n",
    "    \n",
    " \n",
    "    test_=[]\n",
    "    for line in fin2:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        test_.append(sentence)\n",
    "    \n",
    "    for te in test_:\n",
    "        sentence_text=list(te[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        entity_list=[]\n",
    "        entitypair_list=[]\n",
    "        \n",
    "        for entity in te[\"entityMentions\"]:\n",
    "            \n",
    "            if entity['label']=='Nh' or entity['label']=='NDR':\n",
    "                \n",
    "                entity_list.append((entity['text'],entity['start']))\n",
    "           \n",
    "#             entity_list.append((entity['text'],entity['start']))\n",
    "        for e1 in entity_list:\n",
    "            for e2 in entity_list:\n",
    "                if e1!=e2:\n",
    "                    entitypair=set()\n",
    "                    entitypair.add(e1)\n",
    "                    entitypair.add(e2)\n",
    "                    if entitypair not in entitypair_list:\n",
    "                        entitypair_list.append(entitypair)\n",
    "                       \n",
    "        for relation in te['relationMentions']:\n",
    "            if relation['label']!='NA':\n",
    "                sent=sentence_text.copy()\n",
    "\n",
    "                e1=relation['em1Text']\n",
    "                e2=relation['em2Text']\n",
    "                re=relation['label']\n",
    "\n",
    "                re_pair=set()\n",
    "                re_pair.add(e1)\n",
    "                re_pair.add(e2)\n",
    "\n",
    "                if re_pair in entitypair_list:\n",
    "                    entitypair_list.remove(re_pair)\n",
    "                te['relationMentions'].append({\"e1start\":relation['e21start'],\\\n",
    "                                               \"em1Text\":relation['em2Text'],\\\n",
    "                                               \"e21start\":relation['e1start'],\\\n",
    "                                               \"em2Text\":relation['em1Text'],\\\n",
    "                                               \"label\":'NA'})\n",
    "#         print(1,len(te['relationMentions']))\n",
    "        if len(entitypair_list)>5:\n",
    "            ranlist=entitypair_list\n",
    "            ranlist=random.sample(entitypair_list,5)\n",
    "            \n",
    "            for ran in ranlist:\n",
    "                te['relationMentions'].append({\"e1start\":list(ran)[0][1],\\\n",
    "                                               \"em1Text\":list(ran)[0][0],\\\n",
    "                                               \"e21start\":list(ran)[1][1],\\\n",
    "                                               \"em2Text\":list(ran)[1][0],\\\n",
    "                                               \"label\":'NA'})\n",
    "\n",
    "#         else:\n",
    "#             for ran in entitypair_list:\n",
    "#                 tr['relationMentions'].append({\"e1start\":list(ran)[0][1],\\\n",
    "#                                                \"em1Text\":list(ran)[0][0],\\\n",
    "#                                                \"e21start\":list(ran)[1][1],\\\n",
    "#                                                \"em2Text\":list(ran)[1][0],\\\n",
    "#                                                \"label\":'NA'})\n",
    "#         print(len(te['relationMentions']))\n",
    "        fout2.write(json.dumps(te,ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['traffic_in', 'posess', 'sell_drugs_to', 'provide_shelter_for', 'NA']\n"
     ]
    }
   ],
   "source": [
    "with open('../data/train_new.json','rt',encoding='utf-8') as fin1,open('../data/test_new.json','rt',encoding='utf-8') as fin2,\\\n",
    "open('../data/train.tsv','w',newline='',encoding='utf-8') as fout1,open('../data/dev.tsv','w',newline='',encoding='utf-8') as fout2,open('../data/test.tsv','w',newline='',encoding='utf-8') as fout3:\n",
    "    wtrain=csv.writer(fout1,delimiter='\\t')\n",
    "    wvalid=csv.writer(fout2,delimiter='\\t')\n",
    "    wtest=csv.writer(fout3,delimiter='\\t')\n",
    "    sid=0\n",
    "    train_=[]\n",
    "    for line in fin1:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        train_.append(sentence)\n",
    "    train_size=len(train_)\n",
    "    val_size = int(0.25 * len(train_))\n",
    "    train_,val_= random_split(train_, [train_size-val_size,val_size])\n",
    "    train_=list(train_)\n",
    "    val_=list(val_)\n",
    "    \n",
    "    \n",
    "    fre=open('relations.txt','r',encoding='utf-8') \n",
    "    relation_type=[x.strip() for x in fre.readlines()]\n",
    "    \n",
    "    print(relation_type)\n",
    "    \n",
    "    for tr in train_:\n",
    "        sentence_text=list(tr[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        sentID=str(tr['articleId'])+'-'+str(tr['sentId'])\n",
    "        #print(tr)\n",
    "            \n",
    "        for relation in tr['relationMentions']:\n",
    "            sent=sentence_text.copy()\n",
    "            \n",
    "            e1start=relation['e1start']\n",
    "            e1end=e1start+len(relation['em1Text'])\n",
    "            e2start=relation['e21start']\n",
    "            e2end=e2start+len(relation['em2Text'])\n",
    "            re_type=relation_type.index(relation['label'])\n",
    "            \n",
    "            if e1end>400 or e2end>400:\n",
    "                continue\n",
    "                \n",
    "            if e1start<e2start:\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "            else:\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                \n",
    "            wtrain.writerow((sid,''.join(sent),re_type,sentID))\n",
    "            #print(sid,''.join(sent),re_type,sentID)\n",
    "            sid+=1\n",
    "#             print(sentence_text[j],label[j])\n",
    "    sid=0\n",
    "    for vl in val_:\n",
    "        sentence_text=list(vl[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        sentID=str(vl['articleId'])+'-'+str(vl['sentId'])\n",
    "        #print(tr)\n",
    "        for relation in vl['relationMentions']:\n",
    "            sent=sentence_text.copy()\n",
    "            \n",
    "            e1start=relation['e1start']\n",
    "            e1end=e1start+len(relation['em1Text'])\n",
    "            e2start=relation['e21start']\n",
    "            e2end=e2start+len(relation['em2Text'])\n",
    "            re_type=relation_type.index(relation['label'])\n",
    "            \n",
    "            if e1end>400 or e2end>400:\n",
    "                continue\n",
    "                \n",
    "            if e1start<e2start:\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "            else:\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                \n",
    "            wvalid.writerow((sid,''.join(sent),re_type,sentID))\n",
    "            #print(sid,''.join(sent),re_type,sentID)\n",
    "            sid+=1\n",
    "#             print(sentence_text[j],label[j])\n",
    "    \n",
    "    sid=0\n",
    "    test_=[]\n",
    "    for line in fin2:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  #结束则跳出循环\n",
    "        sentence = json.loads(line)#将json数据转换成python对象  这里是转换为了字典\n",
    "        test_.append(sentence)\n",
    "    \n",
    "    for te in test_:\n",
    "        sentence_text=list(te[\"sentText\"].strip().strip('\"'))\n",
    "#         print(len(sentence_text))\n",
    "        sentID=str(te['articleId'])+'-'+str(te['sentId'])\n",
    "        #print(tr)\n",
    "        for relation in te['relationMentions']:\n",
    "            sent=sentence_text.copy()\n",
    "            \n",
    "            e1start=relation['e1start']\n",
    "            e1end=e1start+len(relation['em1Text'])\n",
    "            e2start=relation['e21start']\n",
    "            e2end=e2start+len(relation['em2Text'])\n",
    "            re_type=relation_type.index(relation['label'])\n",
    "            \n",
    "            if e1end>400 or e2end>400:\n",
    "                continue\n",
    "                \n",
    "            if e1start<e2start:\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "            else:\n",
    "                sent.insert(e1end,'[E12]')\n",
    "                sent.insert(e1start,'[E11]')\n",
    "                sent.insert(e2end,'[E22]')\n",
    "                sent.insert(e2start,'[E21]')\n",
    "                \n",
    "            wtest.writerow((sid,''.join(sent),re_type,sentID))\n",
    "            #print(sid,''.join(sent),re_type,sentID)\n",
    "            sid+=1\n",
    "#             print(sentence_text[j],label[j])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature=torch.load('data2/cached_dev_bert-base-uncased_400_semeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "         \"bert-base-chinese\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '龙',\n",
       " '口',\n",
       " '市',\n",
       " '人',\n",
       " '民',\n",
       " '检',\n",
       " '察',\n",
       " '院',\n",
       " '起',\n",
       " '诉',\n",
       " '书',\n",
       " '指',\n",
       " '控',\n",
       " '，',\n",
       " '2014',\n",
       " '年',\n",
       " '11',\n",
       " '月',\n",
       " '至',\n",
       " '2015',\n",
       " '年',\n",
       " '1',\n",
       " '月',\n",
       " '期',\n",
       " '间',\n",
       " '，',\n",
       " '被',\n",
       " '告',\n",
       " '人',\n",
       " '[UNK]',\n",
       " '高',\n",
       " '某',\n",
       " '[UNK]',\n",
       " '先',\n",
       " '后',\n",
       " '多',\n",
       " '次',\n",
       " '容',\n",
       " '留',\n",
       " '吸',\n",
       " '毒',\n",
       " '人',\n",
       " '员',\n",
       " '[UNK]',\n",
       " '赵',\n",
       " '某',\n",
       " '某',\n",
       " '[UNK]',\n",
       " '、',\n",
       " '[UNK]',\n",
       " '汇',\n",
       " '峰',\n",
       " '[UNK]',\n",
       " '在',\n",
       " '其',\n",
       " '位',\n",
       " '于',\n",
       " '龙',\n",
       " '口',\n",
       " '市',\n",
       " '龙',\n",
       " '口',\n",
       " '经',\n",
       " '济',\n",
       " '开',\n",
       " '发',\n",
       " '区',\n",
       " '中',\n",
       " '国',\n",
       " '银',\n",
       " '行',\n",
       " '家',\n",
       " '属',\n",
       " '楼',\n",
       " '西',\n",
       " '单',\n",
       " '元',\n",
       " '4',\n",
       " '楼',\n",
       " '中',\n",
       " '户',\n",
       " '的',\n",
       " '住',\n",
       " '处',\n",
       " '吸',\n",
       " '食',\n",
       " '甲',\n",
       " '基',\n",
       " '苯',\n",
       " '丙',\n",
       " '胺',\n",
       " '（',\n",
       " '冰',\n",
       " '毒',\n",
       " '）',\n",
       " '。',\n",
       " '2015',\n",
       " '年',\n",
       " '1',\n",
       " '月',\n",
       " '8',\n",
       " '日',\n",
       " '，',\n",
       " '被',\n",
       " '告',\n",
       " '人',\n",
       " '高',\n",
       " '某',\n",
       " '被',\n",
       " '公',\n",
       " '安',\n",
       " '机',\n",
       " '关',\n",
       " '抓',\n",
       " '获',\n",
       " '归',\n",
       " '案',\n",
       " '。',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(feature[0].input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[UNK]',\n",
       " '高',\n",
       " '某',\n",
       " '[UNK]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.convert_ids_to_tokens(feature[0].input_ids*np.array(feature[0].e1_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 7770, 3378, 100]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '高', '某', '[UNK]']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(np.array(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature[0].input_ids*np.array(feature[0].e1_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l=['qwrwe','wetert','er']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x7f2c8babfac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = map(len,l)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20833333333333334 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenyanguang/envs/anaconda3/envs/jointextraction/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score,precision_score,recall_score,accuracy_score\n",
    "x=np.array([1,0,2,3,4,5])\n",
    "y=np.array([1,1,0,1,1,5])\n",
    "w=precision_score(y_true=x, y_pred=y, average='macro')\n",
    "r=recall_score(y_true=x, y_pred=y, average='macro')\n",
    "accuracy_score(y_true=x, y_pred=y)\n",
    "print(w,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 2.]],\n",
      "\n",
      "        [[1., 2., 3.]]]) tensor([[ 0.8127],\n",
      "        [-0.0581],\n",
      "        [ 0.9994]])\n",
      "tensor([[[1.9406, 1.9406, 1.9406]],\n",
      "\n",
      "        [[3.6946, 3.6946, 3.6946]]])\n",
      "tensor([[[1.9406],\n",
      "         [1.9406],\n",
      "         [1.9406]],\n",
      "\n",
      "        [[3.6946],\n",
      "         [3.6946],\n",
      "         [3.6946]]])\n",
      "tensor([[[3.8813, 3.8813, 3.8813],\n",
      "         [3.8813, 3.8813, 3.8813],\n",
      "         [3.8813, 3.8813, 3.8813]],\n",
      "\n",
      "        [[7.3892, 7.3892, 7.3892],\n",
      "         [7.3892, 7.3892, 7.3892],\n",
      "         [7.3892, 7.3892, 7.3892]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3333, 0.3333, 0.3333],\n",
       "         [0.3333, 0.3333, 0.3333],\n",
       "         [0.3333, 0.3333, 0.3333]],\n",
       "\n",
       "        [[0.3333, 0.3333, 0.3333],\n",
       "         [0.3333, 0.3333, 0.3333],\n",
       "         [0.3333, 0.3333, 0.3333]]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "x=torch.tensor([[[0,1,2]],\n",
    "\n",
    "        [[1., 2.,3.]]])\n",
    "y=torch.randn((3,1))\n",
    "\n",
    "print(x,y)\n",
    "\n",
    "a=torch.matmul(x,y).expand(-1, -1, 3)\n",
    "print(a)\n",
    "b=torch.matmul(x,y).expand(-1, -1, 3).transpose(1, 2)\n",
    "print(b)\n",
    "print(a+b)\n",
    "torch.nn.functional.softmax(a+b,dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=set()\n",
    "a.add(1)\n",
    "a.add(2)\n",
    "b=set()\n",
    "b.add(2)\n",
    "b.add(1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:jointextraction]",
   "language": "python",
   "name": "conda-env-jointextraction-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
